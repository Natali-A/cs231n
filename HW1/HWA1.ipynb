{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear Image Classifier\n",
    "\n",
    "In this exercise you will implement a linear image classifier while getting familiar with `numpy` and the benefits of vectorized operations in Python. This exercise has 3 parts:\n",
    "\n",
    "1. Python warmup: working with images, refresher on classes and objects.\n",
    "2. Implementing loss functions, calculating gradients and implementing gradient descent.\n",
    "3. Training and evaluating several classifiers.\n",
    "\n",
    "## Submission guidelines:\n",
    "\n",
    "Your zip should include the following files only:\n",
    "```\n",
    "- HW1.ipynb\n",
    "- functions/\n",
    "    - classifier.py\n",
    "    - losses.py\n",
    "```\n",
    "Name the file `ex1_ID.zip` and do **not** include any additional directories or the data. \n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "2. Write **efficient vectorized** code whenever instructed. \n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Do not change the functions we provided you. \n",
    "4. Write your functions in the instructed python modules only. All the logic you write is imported and used using this jupyter notebook. You are allowed to add functions as long as they are located in the python modules and are imported properly.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden.\n",
    "6. Your code must run without errors. Use `python 3` and `numpy 1.15.4` (configure a new environment as shown in the 2nd lecture if needed). Changes of the configuration we provided are at your own risk. Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works. **Code that cannot run will not be tested.**\n",
    "7. Write your own code. Cheating will not be tolerated. \n",
    "8. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What archive file format should you be using to compress the files and upload them to Moodle?\n",
    "\n",
    "A: \n",
    "\n",
    "Q: Should you run your code and make sure everything works just before you **zip** the files for submission? What will happen if your code cannot run?\n",
    "\n",
    "A:\n",
    "\n",
    "Q: Should you include the data in your submission?\n",
    "\n",
    "A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functions.classifier import LinearPerceptron\n",
    "from functions.classifier import LogisticRegression\n",
    "from functions.losses import perceptron_loss_vectorized\n",
    "from functions.losses import perceptron_loss_naive\n",
    "from functions.losses import binary_cross_entropy\n",
    "from functions.losses import grad_check\n",
    "\n",
    "# specify the way plots behave in jupyter notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Numpy version: \", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "The next few cells will download and extract CIFAR-10 into `datasets/cifar10/`. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The dataset is divided into five training batches and one test batch, each with 10,000 images. The test batch contains exactly 1,000 randomly-selected images from each class.\n",
    "\n",
    "We have included several image processing functions. Notice the following in particular: we created an additional validation dataset you need to use for hyperparameter optimization (learning rate and L2 regularization). We subtracted the mean from all the images in order to ignore illumination conditions while keeping the content of the image. Next, we flattened the images from a tensor of shape (32x32x3) to a vector with 3072 features (pixel values) so we would be able to use a simple matrix multiplication. Finally, we concatenated each image vector with an additional feature to account for the bias. This is known as the bias trick. \n",
    "\n",
    "Make sure you understand this image processing pipeline before diving into the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_cifar10\n",
    "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "PATH = 'datasets/cifar10/' # the script will create required directories\n",
    "load_cifar10.maybe_download_and_extract(URL, PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n",
    "X_train, y_train, X_test, y_test = load_cifar10.load(CIFAR10_PATH) # load the entire data\n",
    "\n",
    "# taking only two classes from the dataset\n",
    "X_train = X_train[np.logical_or(y_train == 0, y_train == 1)]\n",
    "y_train = y_train[np.logical_or(y_train == 0, y_train == 1)]\n",
    "X_test = X_test[np.logical_or(y_test == 0, y_test == 1)]\n",
    "y_test = y_test[np.logical_or(y_test == 0, y_test == 1)]\n",
    "\n",
    "# define a splitting for the data\n",
    "num_training = 10000\n",
    "num_validation = 1000\n",
    "num_testing = 1000\n",
    "\n",
    "# add a validation dataset for hyperparameter optimization\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "mask = range(num_validation)\n",
    "X_val = X_test[mask]\n",
    "y_val = y_test[mask]\n",
    "mask = range(num_validation, num_validation+num_testing)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# float64\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_val = X_val.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "# subtract the mean from all the images in the batch\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# flatten all the images in the batch (make sure you understand why this is needed)\n",
    "X_train = np.reshape(X_train, newshape=(X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, newshape=(X_val.shape[0], -1)) \n",
    "X_test = np.reshape(X_test, newshape=(X_test.shape[0], -1)) \n",
    "\n",
    "# add a bias term to all images in the batch\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]) \n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]) \n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]) \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "classes = ('plane', 'car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, n):\n",
    "    rand_items = np.random.randint(0, X.shape[0], size=n)\n",
    "    images = X[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    return X, y\n",
    "\n",
    "def make_random_grid(x, y, n=4):\n",
    "    rand_items = np.random.randint(0, x.shape[0], size=n)\n",
    "    images = x[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    grid = np.hstack((np.asarray((vec_2_img(i) + mean_image), dtype=np.int) for i in images))\n",
    "    print(' '.join('%13s' % classes[labels[j]] for j in range(4)))\n",
    "    return grid\n",
    "\n",
    "def vec_2_img(x):\n",
    "    x = np.reshape(x[:-1], (32, 32, 3))\n",
    "    return x\n",
    "\n",
    "X_batch, y_batch = get_batch(X_test, y_test, 4)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier: mapping images to scores\n",
    "\n",
    "During this exercise, we will maintain a python class with basic functionality (such as training the model). the linear classifiers we will build (perceptron, logistic regression) will inherit some functionality from that class and will change several functions (such as the loss function, for example). Open the file `functions/classifier.py` and make sure you understand the code. You might find this [short classes in python tutorial](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/) useful.\n",
    "\n",
    "## Linear perceptron\n",
    "Our first linear classifier will include a linear function that maps images to scores:\n",
    "\n",
    "$$\n",
    "f(x_i; W, b) = W\\cdot x_i + b\n",
    "$$\n",
    "\n",
    "As you learned in class, this linear classifier takes an input image $x_i$ and outputs a class score. Your goal is to **learn** the parameters $W$ and $b$ to best classify the images according to the provided labels. The linear perceptron is set up so that the perceptron learn to map the correct class for each image such that it will have a score higher than the incorrect class. In this exercise, we will define our Linear perceptron to have two outputs - one outputs for each class.      \n",
    "\n",
    "Open the file `functions/classifier.py`. The constructor of the `LinearPerceptron` class takes as input the dataset and labels in order to create appropriate parameters. Notice we are using the bias trick and only use the matrix `w` for convenience. Since we already have a (random) model, we can start predicting classes on images. Complete the method `predict` in the `LinearPerceptron` class. **(2.5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = LinearPerceptron(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = get_batch(X_train, y_train, 4)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "print(' '.join('%13s' % classes[y_pred[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Complete the class method `calc_accuracy` in `functions/classifier.py`. Explain why the accuracy on the training dataset (remember, the model is not trained yet) is around 50%. **(2.5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model accuracy: \", classifier.calc_accuracy(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss (30 points)\n",
    "\n",
    "Your code for this section will all be written inside `functions/losses.py`. In this section, we write and test code outside the classes for convenience. Notice the loss method for each class is just a call for the loss function written in `losses.py`. Once you are finished with implementation, everything should work.\n",
    "\n",
    "First, complete the function `perceptron_loss_naive`. This function takes as input the weights, data, labels and a regularization term and outputs the calculated loss as a single number and the gradients with respect to W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(3073, 2) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_naive, grad_naive = perceptron_loss_naive(W, X_val, y_val)\n",
    "print ('loss: %f' % (loss_naive, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with a gradient checking function called `grad_check` in `functions/losses.py`. The following cells test your implementation of the loss value and gradient. Errors should be below $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grad = perceptron_loss_naive(W, X_val, y_val)\n",
    "f = lambda w: perceptron_loss_naive(w, X_val, y_val)[0]\n",
    "grad_numerical = grad_check(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your code works, complete the function `perceptron_loss_vectorized` and compare the results of the two functions using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loss_vectorized, grad_vectorized = perceptron_loss_vectorized(W, X_val, y_val)\n",
    "print ('loss: %f' % (loss, ))\n",
    "\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized)\n",
    "print ('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained an efficient function for loss and gradient calculation and we can now train our network. Complete the function `train` in `functions/classifier.py`. This function should be implemented in the `LinearClassifier` class. (**10 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "perceptron = LinearPerceptron(X_train, y_train)\n",
    "loss_history = perceptron.train(X_train, y_train, \n",
    "                         learning_rate=1e-7,\n",
    "                         num_iters=1500,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", perceptron.calc_accuracy(X_train, y_train))\n",
    "print(\"Testing accuracy: \", perceptron.calc_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Your model should have improved from 50% accuracy to ~75% accuracy in a matter of seconds. Now, use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, batch_size)` to tuples of the form `(training_accuracy, validation_accuracy)`. Finally, you should evaluate the best model on the testing dataset. \n",
    "\n",
    "Note: When changing the batch_size, change the number of iterations accordingly such that the number of epochs on the data stays roughly the same. A reasonable ratio is 600 iterations for a batch size of 200. \n",
    "\n",
    "If you are carful you should reach ~83% accuracy on the validation dataset.\n",
    "\n",
    "Use a small value for the number of iterations as you develop your code. Once you are confident that everything works, run it again for more iterations. Finally, explain the results - what can you learn from the hyper parameters that yields the best results? Why do you think that is the case? **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are encouraged to experiment with additional values\n",
    "learning_rates = [1e-7, 5e-6]\n",
    "batch_sizes = [1, 100, 200, 500, 10000]\n",
    "\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_perceptron = None # The LinearPerceptron object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "#                            START OF YOUR CODE                                #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, batch_size in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, batch_size)]\n",
    "    print ('lr %e batch_size %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, batch_size, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "test_accuracy = best_perceptron.calc_accuracy(X_test, y_test)\n",
    "print ('linear perceptron on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_perceptron.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 2)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car']\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Another choice for a binary classifier is the binary logistic regression classifier. Unlike the perceptron which treats the outputs as uncalibrated and possibly difficult to interpret scores for each class, the binary logistic regression classifier gives a slightly more intuitive output in the form of normalized class probabilities. In this classifier, the function mapping $f(x_i; W, b) = W\\cdot x_i + b$ stays unchanged but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss. In this exercise, we will define our binary logistic regression classifier to have one input.       \n",
    "\n",
    "Open the file `functions/classifier.py`. The constructor of the `LogisticRegression` class takes as input the dataset and labels in order to create appropriate parameters. Notice we are using the bias trick and only use the matrix `w` for convenience. Since we already have a (random) model, we can start predicting classes on images. Complete the method `predict` in the `LogisticRegression` class - remember you need to implement the sigmoid function before you can obtain predictions using your classifier. (**2.5 points**)\n",
    "\n",
    "**Important note**: values passed to the `sigmoid` function can be arbitrarily large or small. When we take the exponent of such values, we might encounter extreme values that might *overflow*. This is known as numerical instability and you should always take care when you use exponent in your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(X_train, y_train)\n",
    "y_pred = logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = get_batch(X_train, y_train, 4)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "print(' '.join('%13s' % classes[y_pred[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Complete the class method `calc_accuracy` in the `LinearClassifier` located in `functions/classifier.py`. (**2.5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model accuracy: \", logistic.calc_accuracy(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross-entropy (30 points)\n",
    "\n",
    "Your code for this section will all be written inside **functions/losses.py**. \n",
    "\n",
    "Complete the function `binary_cross_entropy` using vectorized code. This function takes as input the weights, data, labels and a regularization term and outputs the calculated loss as a single number and the gradients with respect to W. (**10 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(3073, 1) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_naive, grad_naive = binary_cross_entropy(W, X_val, y_val)\n",
    "print ('loss: %f' % (loss_naive, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grad = binary_cross_entropy(W, X_val, y_val)\n",
    "f = lambda w: binary_cross_entropy(w, X_val, y_val)[0]\n",
    "grad_numerical = grad_check(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If implemented correctly, the training procedure you already implemented should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logistic = LogisticRegression(X_train, y_train)\n",
    "loss_history = logistic.train(X_train, y_train, \n",
    "                         learning_rate=1e-7,\n",
    "                         num_iters=1500,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", logistic.calc_accuracy(X_train, y_train))\n",
    "print(\"Testing accuracy: \", logistic.calc_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Your model should have improved from 50% accuracy to ~75% accuracy in a matter of seconds. Now, use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, batch_size)` to tuples of the form `(training_accuracy, validation_accuracy)`. Finally, you should evaluate the best model on the testing dataset. \n",
    "\n",
    "Note: When changing the batch_size, change the number of iterations accordingly such that the number of epochs on the data stays roughly the same. A reasonable ratio is 600 iterations for a batch size of 200. \n",
    "\n",
    "If you are carful you should reach ~83% accuracy on the validation dataset.\n",
    "\n",
    "Use a small value for the number of iterations as you develop your code. Once you are confident that everything works, run it again for more iterations. Finally, explain the results - what can you learn from the hyper parameters that yields the best results? Why do you think that is the case? **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are encouraged to experiment with additional values\n",
    "learning_rates = [1e-7, 5e-6]\n",
    "batch_sizes = [1, 100, 200, 500, 1000, 10000]\n",
    "\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_logistic = None # The LogisticRegression object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "#                            START OF YOUR CODE                                #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, batch_size in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, batch_size)]\n",
    "    print ('lr %e batch_size %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, batch_size, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "test_accuracy = best_perceptron.calc_accuracy(X_test, y_test)\n",
    "print ('Binary logistic regression on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_logistic.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 1)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['car']\n",
    "for i in range(1):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the possible reasons for the differences in the visualization of the weights in the both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus (**10 points**): Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term to the loss to penalize larger weights. For this part, implement L2 regularization of the form:\n",
    "$$\n",
    "Loss = Loss + \\lambda \\cdot \\sum_{i=0}^k w_k^2\n",
    "$$\n",
    "Where $\\lambda$ is yet another hyper parameter. Search for an optimal $\\lambda$ (look around 5e4) and don't forget to update the gradient or the regularization won't effect the weights. When you are finished, train a perceptron classifier and visualize the weights. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
